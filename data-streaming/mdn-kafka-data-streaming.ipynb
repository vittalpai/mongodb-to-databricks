{"cells":[{"cell_type":"markdown","source":["## MongoDB - Kafka Data Streaming\n\nThe following notebook illustrates how to setup continuos streaming of the data from MongoDB via to Databricks Delta Lake\n\nUpdate the variable **TOPIC** , **KAFKA_BROKER**, **API_KEY** & **SECRET** with relevant details."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58c949ba-32b7-4154-a124-789c53c5b7ad","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import dlt\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nTOPIC = \"<DB.COLLECTION>\";\nKAFKA_BROKER = \"hostname:port1, hostname:port2\";  // comma separated list of broker:host\nAPI_KEY = \"\";\nSECRET = \"\";\n\n\nraw_kafka_events = (spark.readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)  \n    .option(\"subscribe\", TOPIC)\n    .option(\"kafka.security.protocol\", \"SASL_SSL\")\n    .option(\"kafka.sasl.jaas.config\", \"kafkashaded.org.apache.kafka.common.security.plain.PlainLoginModule required username='{}' password='{}';\".format(API_KEY, SECRET))\n    .option(\"kafka.ssl.endpoint.identification.algorithm\", \"https\")\n    .option(\"kafka.sasl.mechanism\", \"PLAIN\")\n    .option(\"failOnDataLoss\", \"false\")\n    .option(\"startingOffsets\", \"earliest\")\n    .load())\n\n\n@dlt.table(table_properties={\"pipelines.reset.allowed\":\"false\"})\ndef mdb_data():\n  return raw_kafka_events"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dece0799-9092-4a2f-b2da-66f87800e25e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MDB - Kafka Data Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1179426203024693}},"nbformat":4,"nbformat_minor":0}
