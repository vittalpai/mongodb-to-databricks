{"cells":[{"cell_type":"markdown","source":["## MongoDB - Kafka Data Streaming\n\nThe following notebook illustrates how to setup continuos streaming of the data from MongoDB via to Databricks Delta Lake\n\nUpdate the **TOPIC** and **KAFKA_BROKER** with relevant details."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58c949ba-32b7-4154-a124-789c53c5b7ad","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["import dlt\nfrom pyspark.sql.functions import *\nfrom pyspark.sql.types import *\n\nTOPIC = \"tracker-events\"\nKAFKA_BROKER = \"hostname:port1, hostname:port2\";  // comma separated list of broker:host\n\n\nraw_kafka_events = (spark.readStream\n    .format(\"kafka\")\n    .option(\"kafka.bootstrap.servers\", KAFKA_BROKER)  \n    .option(\"subscribe\", TOPIC)\n    .option(\"startingOffsets\", \"earliest\")\n    .load())\n\n\n@dlt.table(table_properties={\"pipelines.reset.allowed\":\"false\"})\ndef mdb_data():\n  return raw_kafka_events"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dece0799-9092-4a2f-b2da-66f87800e25e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MDB - Kafka Data Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1179426203024693}},"nbformat":4,"nbformat_minor":0}
