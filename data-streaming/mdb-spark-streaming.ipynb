{"cells":[{"cell_type":"markdown","source":["## MongoDB Atlas Spark Streaming\n\nThe following illustrates how to setup continuos streaming of the data from MongoDB to Databricks Delta Lake.\n\n### Create a Databricks Cluster and Add the Connector as a Library\n\n1. Create a Databricks cluster.\n2. Download the latest version(>10.1) of MongoDB connector for Apache spark jar file from [Maven Central](https://repo1.maven.org/maven2/org/mongodb/spark/)\n   **Note**: Make sure the package has same version of Scala as the Scala version of the Spark Pool.\n3. Navigate to the cluster detail page and select the **Libraries** tab.\n4. Click the **Install New** button.\n5. Select **Upload** as the Library Source and **JAR** as the library type.\n6. Drop the MongoDB connector JAR downloaded from the step 2.\n7. Click **Install**. <br/>\nFor more info on the MongoDB Spark connector (which now supports structured streaming) see the [MongoDB documentation](https://www.mongodb.com/docs/spark-connector/current/). \n\n### Create a MongoDB Atlas Instance\n\nAtlas is a fully managed, cloud-based MongoDB service. We'll use Atlas to test the integration between MongoDb and Spark.\n\n1. Sign up for [MongoDB Atlas](https://www.mongodb.com/atlas/database). \n2. [Create an Atlas free tier cluster](https://docs.atlas.mongodb.com/getting-started/).\n3. Enable Databricks clusters to connect to the cluster by adding the external IP addresses for the Databricks cluster nodes to the [whitelist in Atlas](https://docs.atlas.mongodb.com/setup-cluster-security/#add-ip-addresses-to-the-whitelist). For convenience you could (**temporarily!!**) 'allow access from anywhere', though we recommend to enable [network peering](https://www.mongodb.com/docs/atlas/security-vpc-peering/) for production. \n\n### Prep MongoDB with a sample data-set \n\nMongoDB comes with a nice sample data-set that allows to quickly get started. We will use this in the context of this notebook\n\n1. In MongoDB Atlas [Load the sample data-set](https://www.mongodb.com/docs/charts/tutorial/order-data/prerequisites-setup/) once the cluster is up and running. \n2. You can confirm the presence of the data-set via the **Browse Collections** button in the Atlas UI.\n\n### Update Spark Configuration with the Atlas Connection String\n\n\n1. Note the connect string under the **Connect** dialog in MongoDB Atlas. It has the form of \"mongodb+srv://\\<username>\\:\\<password>\\@\\<databasename>\\.xxxxx.mongodb.net/\"\n2. Update the Mongodb connection string below."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"58c949ba-32b7-4154-a124-789c53c5b7ad","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["connectionString='mongodb+srv://CONNECTION_STRING_HERE/\ndatabase=\"sample_supplies\"\ncollection=\"sales\"\ndestination_table=\"deltalake_sales\""],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"122a3d56-8180-472b-9d7e-794fd43403e8","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Create a Spark Readstream\n\nYou will need to create a readstream in order to use Spark with the MongoDB Connector."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"138757e6-ab7b-42b8-9376-0fa5076adcdd","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["query=(spark.readStream.format(\"mongodb\").\\\n\toption('spark.mongodb.connection.uri', connectionString).\\\n\toption('spark.mongodb.database', database).\\\n\toption('spark.mongodb.collection', collection).\\\n\toption('spark.mongodb.change.stream.publish.full.document.only','true').\\\n\toption(\"forceDeleteTempCheckpointLocation\", \"true\").\\\n\tload())"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"8c74f544-f051-4665-88e2-aaafdb5ace9e","inputWidgets":{},"title":""}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#### Write data to Delta lake \n\n\nOnce you have the data in a Spark readstream, you can use the writeStream method to write the data to Delta lake in a table called deltalake_sales."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"2575fde1-e898-4cee-9535-fffa5e891ba4","inputWidgets":{},"title":""}}},{"cell_type":"code","source":["query.writeStream.format(\"delta\").\\\n    outputMode(\"append\").\\\n    option(\"checkpointLocation\", \"/tmp/delta/_checkpoint/\").\\\n    option(\"path\", \"/delta/deltalake_sales\").\\\n    table(destination_table)"],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"e34f7825-3d71-49e1-b9fb-996e508d1e9d","inputWidgets":{},"title":""}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"name":null,"datasetInfos":[],"data":"Out[11]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f780b776040>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[11]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f780b776040>"]}},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"data":"java.lang.NoClassDefFoundError: Could not initialize class com.mongodb.spark.sql.connector.read.MongoScanBuilder\n\tat com.mongodb.spark.sql.connector.MongoTable.newScanBuilder(MongoTable.java:125)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:155)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:149)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:133)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.planQuery(MicroBatchExecution.scala:133)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:259)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:259)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:363)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:344)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$4(StreamExecution.scala:269)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:104)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:269)\n","errorSummary":"ERROR: Some streams terminated before this command could finish!","metadata":{},"errorTraceType":"raw","type":"ipynbError","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\njava.lang.NoClassDefFoundError: Could not initialize class com.mongodb.spark.sql.connector.read.MongoScanBuilder\n\tat com.mongodb.spark.sql.connector.MongoTable.newScanBuilder(MongoTable.java:125)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.$anonfun$applyOrElse$4(MicroBatchExecution.scala:155)\n\tat scala.collection.mutable.HashMap.getOrElseUpdate(HashMap.scala:86)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:149)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution$$anonfun$1.applyOrElse(MicroBatchExecution.scala:133)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.$anonfun$transformDownWithPruning$1(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:99)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDownWithPruning(TreeNode.scala:512)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.org$apache$spark$sql$catalyst$plans$logical$AnalysisHelper$$super$transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning(AnalysisHelper.scala:268)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper.transformDownWithPruning$(AnalysisHelper.scala:264)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.transformDownWithPruning(LogicalPlan.scala:31)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transformDown(TreeNode.scala:488)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.transform(TreeNode.scala:456)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.planQuery(MicroBatchExecution.scala:133)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan$lzycompute(MicroBatchExecution.scala:259)\n\tat org.apache.spark.sql.execution.streaming.MicroBatchExecution.logicalPlan(MicroBatchExecution.scala:259)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.$anonfun$runStream$1(StreamExecution.scala:363)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat org.apache.spark.sql.SparkSession.withActive(SparkSession.scala:985)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution.org$apache$spark$sql$execution$streaming$StreamExecution$$runStream(StreamExecution.scala:344)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.$anonfun$run$4(StreamExecution.scala:269)\n\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.java:23)\n\tat com.databricks.unity.EmptyHandle$.runWithAndClose(UCSHandle.scala:104)\n\tat org.apache.spark.sql.execution.streaming.StreamExecution$$anon$1.run(StreamExecution.scala:269)"]}}],"execution_count":0},{"cell_type":"markdown","source":["### Test data streaming from MongoDB Atlas to the Databricks platform\nYou are now ready to go, let's test the data streaming capability from MongoDB Atlas to the Databricks platform.\n\n1. Execute this notebook on the cluster you created.\n2. Add or modify any document in the MongoDB Collection `sales`.\n3. You should observe the corresponding changes reflected in the `deltalake_sales` delta lake table on the Databricks platform.\n\n## More Info\n\n- Discover more about the configuration of the MongoDB Spark Connector [here](https://www.mongodb.com/docs/spark-connector/current/)."],"metadata":{"application/vnd.databricks.v1+cell":{"showTitle":false,"cellMetadata":{},"nuid":"dff6a2da-f311-4e17-a975-e2be7d40bf9b","inputWidgets":{},"title":""}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"MDB - Spark Streaming","dashboards":[],"notebookMetadata":{"pythonIndentUnit":4},"language":"python","widgets":{},"notebookOrigID":1179426203024693}},"nbformat":4,"nbformat_minor":0}
